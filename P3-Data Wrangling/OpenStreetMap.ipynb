{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Problems Encountered in the Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project I had initially chosen data from Brasilia, which is the political capital of Brazil and also my hometown. But an initial analysis showed very little information available for Brasilia (there were zero entries with address data). \n",
    "\n",
    "I've then switched to Rio de Janeiro, the second largest city in Brazil, known for its beautiful beaches and carnival celebrations. According to wikipedia, the municipality of Rio de Janeiro has an area of 1.2 million square kilometers, and 6.5 million inhabitants(12 million when you include metropolitan area)\n",
    "\n",
    "The data was download from Mapzen, size file is 243 mb. The code in wrangling.py is being used to clean up the data, and convert to a json format ready, document database ready (code is based on lesson 6). \n",
    "\n",
    "Let's import the data, and take a first look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import print_function\n",
    "from pprint import pprint\n",
    "import random\n",
    "import re \n",
    "from collections import *\n",
    "\n",
    "%run wrangling.py\n",
    "\n",
    "#setup the connection\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "db = client['osm']\n",
    "coll = db['rio-de-janeiro']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-10-26T23:52:10.827-0200\tconnected to: localhost\n",
      "2015-10-26T23:52:13.821-0200\t[###.....................] osm.rio-de-janeiro\t42.5 MB/276.3 MB (15.4%)\n",
      "2015-10-26T23:52:16.821-0200\t[#######.................] osm.rio-de-janeiro\t87.1 MB/276.3 MB (31.5%)\n",
      "2015-10-26T23:52:19.822-0200\t[###########.............] osm.rio-de-janeiro\t131.1 MB/276.3 MB (47.4%)\n",
      "2015-10-26T23:52:22.824-0200\t[###############.........] osm.rio-de-janeiro\t174.8 MB/276.3 MB (63.3%)\n",
      "2015-10-26T23:52:25.821-0200\t[##################......] osm.rio-de-janeiro\t218.4 MB/276.3 MB (79.1%)\n",
      "2015-10-26T23:52:28.821-0200\t[#######################.] osm.rio-de-janeiro\t265.4 MB/276.3 MB (96.1%)\n",
      "2015-10-26T23:52:29.484-0200\timported 1347275 documents\n"
     ]
    }
   ],
   "source": [
    "# process data, save to json and import data using shell (faster)\n",
    "process_map('rio-de-janeiro_brazil.osm')\n",
    "db['rio-de-janeiro'].drop()\n",
    "!mongoimport --db osm --collection rio-de-janeiro --file rio-de-janeiro_brazil.osm.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries:  1347275\n",
      "Total entries with address:  14921\n",
      "0.0110749475794\n"
     ]
    }
   ],
   "source": [
    "#let's run some queries to see how complete is the data\n",
    "#check how many entries there are, and how many of those have address\n",
    "cursor = coll.find()\n",
    "print('Total entries: ', cursor.count())\n",
    "cursor = coll.find({'address': {'$exists': 1}})\n",
    "print('Total entries with address: ', cursor.count())\n",
    "print(14921/1347275.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are not many entries with address, only 14,921 out of 1,347,275. That is a little over 1%. But is still far better than Brasilia, which had 0 entries with address. Let's take a look at the addresses, see if there is any fixing required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city :  Rio de Janeiro\n",
      "country :  BR\n",
      "state :  RJ\n",
      "street :  PraÃ§a Senador Salgado Filho\n",
      "postcode :  20021-340\n",
      "continent :  South America\n",
      "\n",
      "country :  BR\n",
      "street :  Rua do Catete\n",
      "housenumber :  153\n",
      "postcode :  22220-000\n",
      "city :  Rio de Janeiro\n",
      "\n",
      "city :  Rio de Janeiro\n",
      "street :  Rua do Catete\n",
      "housenumber :  104\n",
      "country :  BR\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#quick look at addresses to see overall format \n",
    "cursor = coll.find({'address.street': {'$exists': 1}}) \n",
    "for row in cursor.limit(3):\n",
    "    for k,v in row['address'].items():\n",
    "        print(k,\": \",v)\n",
    "    print(\"\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information relevant to us is street number, house number and post code. Let's take a further look at these three. In portuguese, the first word of the street indicates its type (Rua means \"Street\", for example). So let's isolate all possible types of street and see if they are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Boulevard': 5, u'Caminho': 28, u'Parque': 2, u'Ladeira': 13, u'Alameda': 5, u'Dias': 2, u'Estrada': 1549, u'Praca': 2, u'Rua': 8408, u'Praia': 80, u'Beco': 6, u'Campo': 2, u'Professor': 2, u'rua': 2, u'M': 5, u'P': 2, u'R': 7, u'Alfredo': 5, u'Largo': 20, u'Via': 20, u'Pra': 209, u'Rod': 8, u'Rodovia': 24, u'Travessa': 55, u'Av': 64, u'Avenida': 2568}\n"
     ]
    }
   ],
   "source": [
    "cursor = coll.find({'address.street': {'$exists': 1}}) \n",
    "prefixes = Counter()\n",
    "pattern = re.compile('[\\w]*')\n",
    "for row in cursor:\n",
    "    prefix = re.search(pattern,row['address']['street']).group(0)\n",
    "    prefixes[prefix] += 1 \n",
    "\n",
    "#remove the ocurrences that appear only once\n",
    "prefixes = {k:v for k,v in prefixes.items() if v>1}\n",
    "print(prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks good, overall. But we could reduce the number of the known inconsistencies, such as lower case and abbreviatons. The following code traverses through the findings, and whenever there are known inconsistencies, replace them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "st_name_dict = {'rua': 'Rua', 'P': 'Praia', 'Pra': 'Praia', 'Rod': 'Rodovia', 'Av': 'Avenida', 'R': 'Rua'}\n",
    "cursor = coll.find({'address.street': {'$exists': 1}}) \n",
    "\n",
    "#traverse through results. If found in dict, update\n",
    "for row in cursor:\n",
    "    street = row['address']['street']\n",
    "    prefix = re.search(pattern,street).group(0)    \n",
    "    if prefix in st_name_dict.keys():\n",
    "        updated_st_name = street.replace(prefix, st_name_dict[prefix])\n",
    "        coll.update({'_id': row['_id']}, {'$set': {'address.street': updated_st_name}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'Boulevard': 5, u'Estrada': 1549, u'Praca': 2, u'Campo': 2, u'Caminho': 28, u'Parque': 2, u'Rua': 8417, u'M': 5, u'Rodovia': 32, u'Praia': 291, u'Ladeira': 13, u'Via': 20, u'Alfredo': 5, u'Beco': 6, u'Travessa': 55, u'Professor': 2, u'Largo': 20, u'Avenida': 2632, u'Alameda': 5, u'Dias': 2}\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "cursor = coll.find({'address.street': {'$exists': 1}}) \n",
    "prefixes = Counter()\n",
    "pattern = re.compile('[\\w]*')\n",
    "for row in cursor:\n",
    "    prefix = re.search(pattern,row['address']['street']).group(0)\n",
    "    prefixes[prefix] += 1 \n",
    "\n",
    "#remove the ocurrences that appear only once\n",
    "prefixes = {k:v for k,v in prefixes.items() if v>1}\n",
    "print(prefixes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at post code. The correct pattern in Brazil is xxxxx-xxx. Let's first look for the inconsistencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'22240004', u'20270060', u'25660004', u'22280030', u'22270010', u'24346030', u'2261001', u'24324270', u'22410000', u'22281020', u'20510180', u'25010060', u'26130130', u'26130130', u'26130130', u'26130130', u'26130130', u'25.620-003', u'20720010', u'25963082', u'26953201', u'25958060', u'25953671', u'25955240', u'35953060', u'2695307', u'24230', u'23025 520', u'20770240', u'20775090', u'20550200', u'20550200', u'24744520', u'21921000', u'22440040', u'22440040', u'22.776-070', u'26130230', u'26130230', u'21311050', u'20745000', u'26900000', u'24020000', u'21941005', u'21515090', u'20090-00', u'22430090', u'24220480', u'24060037', u'24060037', u'24060037', u'24060037', u'21230354', u'22631030', u'20770\\u2011001', u'22440033', u'22750009', u'20216005', u'52645100', u'2246-000', u'22471340', u'22471340', u'22471340', u'22471270', u'22471340', u'22471220', u'22471220', u'22471220', u'22471340', u'22471340', u'22471340', u'22471350', u'21920225', u'22071100', u'22795255', u'22735030', u'22775053', u'22775053', u'20771580', u'2785-580', u'20550200', u'20770240', u'24230000', u'21.040-900', u'22723002', u'20.530-001', u'24220401', u'22743050', u'2056005']\n"
     ]
    }
   ],
   "source": [
    "#check for inconsistencies on postcode\n",
    "cursor = coll.find({'address.postcode': {'$exists': 1}}) \n",
    "pattern = re.compile('\\d{5}-\\d{3}')\n",
    "inconsistencies = []\n",
    "for row in cursor:\n",
    "    postcode = row['address']['postcode']\n",
    "    if not re.search(pattern,postcode):\n",
    "        inconsistencies.append(postcode)\n",
    "\n",
    "print(inconsistencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many variations. Mostly are just numbers, without the dash between the fifth and sixth digit. Some are missing a number, and some use a different notation with a dot between the second and third number.\n",
    "\n",
    "To fix this, we can strip everything which is not a number, and then join together with a dash at the appropriate position. If there is less or more than 8 digits, we shall delete the attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#check for inconsistencies on postcode\n",
    "cursor = coll.find({'address.postcode': {'$exists': 1}}) \n",
    "for row in cursor:\n",
    "    original = row['address']['postcode']\n",
    "    digits = re.findall('\\d', original)\n",
    "    length = len(digits)\n",
    "    digits.insert(5, '-')\n",
    "    new = ''.join(digits)\n",
    "    if length != 8:\n",
    "        updated = coll.update({'_id': row['_id']}, {'$unset': {'address.postcode': ''}})        \n",
    "    elif new != original:\n",
    "        updated = coll.update({'_id': row['_id']}, {'$set': {'address.postcode': new}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "cursor = coll.find({'address.postcode': {'$exists': 1}}) \n",
    "pattern = re.compile('\\d{5}-\\d{3}')\n",
    "inconsistencies = []\n",
    "for row in cursor:\n",
    "    postcode = row['address']['postcode']\n",
    "    if not re.search(pattern,postcode):\n",
    "        inconsistencies.append(postcode)\n",
    "\n",
    "print(inconsistencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "House numbers are more complicated to handle. It may contain numbers, letters and symbols in different variations, and so there is not a pattern we can aim for. With that in mind, we will leave house numbers unchanged.\n",
    "\n",
    "As a final check, let's take a look at the main streets and post codes, and see if they look consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main post codes\n",
      "[{u'count': 96, u'_id': u'22720-410'}, {u'count': 84, u'_id': u'22471-003'}, {u'count': 76, u'_id': u'22720-400'}, {u'count': 62, u'_id': u'22220-000'}, {u'count': 56, u'_id': u'22221-000'}]\n",
      "Main streets\n",
      "[{u'count': 511, u'_id': u'Estrada dos Bandeirantes'}, {u'count': 498, u'_id': u'Avenida das Am\\xe9ricas'}, {u'count': 175, u'_id': u'Avenida dos Mananciais'}, {u'count': 153, u'_id': u'Avenida L\\xfacio Costa'}, {u'count': 122, u'_id': u'Avenida Genaro de Carvalho'}]\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {'$match': {'address.postcode': {'$exists': 1}} },\n",
    "    {'$group': {'_id': '$address.postcode', 'count': {'$sum': 1} } },\n",
    "    {'$sort': {'count': -1} },\n",
    "    {'$limit': 5}\n",
    "]\n",
    "groupby_postalcode = [doc for doc in coll.aggregate(pipeline)]\n",
    "\n",
    "pipeline = [\n",
    "    {'$match': {'address.street': {'$exists': 1}} },\n",
    "    {'$group': {'_id': '$address.street', 'count': {'$sum': 1} } },\n",
    "    {'$sort': {'count': -1} },\n",
    "    {'$limit': 5}\n",
    "]\n",
    "groupby_street = [doc for doc in coll.aggregate(pipeline)]\n",
    "\n",
    "print('Main post codes')\n",
    "print(groupby_postalcode)\n",
    "print('Main streets')\n",
    "print(groupby_street)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data on streets and postcodes do not have outliers. There are a number of postal codes and streets with only one address, but it is highly possible that represents the lack of available information, rather than an input error. \n",
    "\n",
    "Finally, let's look at which tags there are available. Let's go through each element of the collection, and count the ocurrences of each type of tag. This information will guide the next step, data analysis, by showing us exactly what information we can extract from this dataset.\n",
    "\n",
    "We will divide in three groups: general attributes, address attributes and created attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cursor = coll.find() \n",
    "address_attrib = Counter()\n",
    "created_attrib = Counter()\n",
    "general_attrib = Counter()\n",
    "\n",
    "for row in cursor:\n",
    "    for elem in row.keys():\n",
    "        if elem == 'address':\n",
    "            for subelem in row[elem]:\n",
    "                address_attrib[subelem] += 1\n",
    "        elif elem == 'created':\n",
    "            for subelem in row[elem]:\n",
    "                created_attrib[subelem] += 1            \n",
    "        else:\n",
    "            general_attrib[elem] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General\n",
      "{u'IPP:BAIRRO': 1253,\n",
      " u'IPP:CHAVE': 1221,\n",
      " u'IPP:CODATIVIDA': 1256,\n",
      " u'IPP:CODBAIRRO': 1252,\n",
      " u'IPP:CODLINFERR': 1188,\n",
      " u'IPP:COD_INEP': 1251,\n",
      " u'IPP:COD_SMA': 1222,\n",
      " u'IPP:CRE': 1256,\n",
      " u'IPP:CodFavela': 1013,\n",
      " u'IPP:DESIGNACAO': 1256,\n",
      " u'IPP:ENDNOVO': 1256,\n",
      " u'IPP:FLG_VALIDA': 1137,\n",
      " u'_id': 1347275,\n",
      " u'id': 1347275,\n",
      " u'node_refs': 132366,\n",
      " u'pos': 1214909,\n",
      " u'type': 1347275}\n",
      "Created\n",
      "{u'changeset': 1347275,\n",
      " u'timestamp': 1347275,\n",
      " u'uid': 1347275,\n",
      " u'user': 1347275,\n",
      " u'version': 1347275}\n",
      "Address:\n",
      "{u'city': 3238,\n",
      " u'country': 1422,\n",
      " u'housenumber': 11111,\n",
      " u'inclusion': 1399,\n",
      " u'interpolation': 1683,\n",
      " u'postcode': 2344,\n",
      " u'street': 13119}\n"
     ]
    }
   ],
   "source": [
    "#filter and print only those greater than 1000\n",
    "treshold = 1000\n",
    "print('General')\n",
    "pprint({k:v for k,v in general_attrib.items() if v>treshold})\n",
    "print('Created')\n",
    "pprint({k:v for k,v in created_attrib.items() if v>treshold})\n",
    "print('Address:')\n",
    "pprint({k:v for k,v in address_attrib.items() if v>treshold})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview                                                \n",
    "This section contains basic statistics about the dataset and the MongoDB queries used to gather them.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File sizes:                                                  \n",
    "rio-de-janeiro_brazil.osm ......... 248 MB  \n",
    "rio-de-janeiro_brazil.osm.json .... 276 MB  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1347275"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents                                                \n",
    "coll.find().count()                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1214909"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of nodes                                                \n",
    "coll.find({\"type\":\"node\"}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132366"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of ways\n",
    "coll.find({\"type\":\"way\"}).count()                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The division between nodes and ways is 90% to 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1214909"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of posts with position\n",
    "coll.find({\"pos\": {\"$exists\": 1}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14921"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of posts with address\n",
    "coll.find({\"address\": {\"$exists\": 1}}).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of unique users                                                \n",
    "len(coll.distinct('created.user'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'count': 379891, u'_id': u'Alexandrecw'}\n",
      "{u'count': 178198, u'_id': u'AlNo'}\n",
      "{u'count': 107435, u'_id': u'ThiagoPv'}\n",
      "{u'count': 89013, u'_id': u'Import Rio'}\n",
      "{u'count': 74735, u'_id': u'Geaquinto'}\n",
      "{u'count': 69132, u'_id': u'Nighto'}\n",
      "{u'count': 55617, u'_id': u'Thundercel'}\n",
      "{u'count': 23687, u'_id': u'bmog'}\n",
      "{u'count': 23248, u'_id': u'user_401472'}\n",
      "{u'count': 23062, u'_id': u'Skippern'}\n",
      "total posts for top 10 users:  1024018\n"
     ]
    }
   ],
   "source": [
    "# Top 10 contributing users\n",
    "pipeline = [\n",
    "        {\"$group\": {\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, \n",
    "        {\"$sort\": {\"count\": -1}}, \n",
    "        {\"$limit\": 10}\n",
    "        ]\n",
    "cursor = coll.aggregate(pipeline)\n",
    "posts_count = 0\n",
    "for row in cursor:\n",
    "    print(row)\n",
    "    posts_count+=row['count']\n",
    "    \n",
    "print('total posts for top 10 users: ', posts_count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'num_users': 182, u'_id': 1}\n",
      "{u'num_users': 86, u'_id': 2}\n",
      "{u'num_users': 47, u'_id': 3}\n",
      "{u'num_users': 32, u'_id': 4}\n",
      "{u'num_users': 37, u'_id': 5}\n",
      "{u'num_users': 33, u'_id': 6}\n",
      "{u'num_users': 21, u'_id': 7}\n",
      "{u'num_users': 22, u'_id': 8}\n",
      "{u'num_users': 22, u'_id': 9}\n",
      "{u'num_users': 15, u'_id': 10}\n",
      "total users with 10 or less posts:  497\n"
     ]
    }
   ],
   "source": [
    "# Number of users per number of posts\n",
    "pipeline = [{\"$group\":{\"_id\":\"$created.user\", \"count\":{\"$sum\":1}}}, \n",
    "            {\"$group\":{\"_id\":\"$count\", \"num_users\":{\"$sum\":1}}}, \n",
    "            {\"$sort\":{\"_id\":1}}, \n",
    "            {\"$limit\":10}\n",
    "           ]\n",
    "cursor = coll.aggregate(pipeline)\n",
    "users_count = 0\n",
    "for row in cursor:\n",
    "    print(row)\n",
    "    users_count += row['num_users']\n",
    "\n",
    "print('total users with 10 or less posts: ', users_count)\n",
    "\n",
    "#_id represents post count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 50% of the users have 10 or less posts, and close to 20% have only one post. \n",
    "\n",
    "The top 10 contributing users list shows there is a high concentration of posts between few top users. The top 1st contributing user has 16 times more posts than the 8th contributing user. \n",
    "\n",
    "The top user alone is responsible for 28% of the total edits. The top10 are responsible for 76% of the total posts. Those numbers probably result from automated ways of inputting data into OSM database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Ideas\n",
    "\n",
    "There is few useful data available apart from latitude and longitude. \n",
    "Only 14,921, or 1,1% of the posts, have information on address. Other non basic tags are present in around a thousand posts each, which is less than 0,1% of the posts count. \n",
    "\n",
    "Open Street Map, although a great initiative, is not known in Brazil. This is the first time I've heard of it. \n",
    "\n",
    "Upon a first look at the website, I could not find an easy way to contribute. It would be of great help if the community had an easier way of mapping the data, possibly by a mobile app that captures as much information as possible (it can get geolocation from the phone gps, for example), and the user only has to fill in the missing parts.\n",
    "\n",
    "As suggested in the example, this mobile app can be gamified to encourage usage, adding features such as points, ranking, badges, reputation, self promotion, and extra perks.\n",
    "\n",
    "The benefits of improving OSM data for the developer, business owner and final user, should also be clearly stated. Any attempt of gathering more users should be preceded by a marketing campaign, starting on its own website, and usings its social net to spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional data exploration using MongoDB queries\n",
    "\n",
    "On the first part, we took a comprehensive look at the information available by mapping the number of ocurrences of each tag. In the result we can see that there are not a lot of tags that could be used to further explore the data and make inferences. \n",
    "\n",
    "Nevertheless, let's look at the most populated streets located on suburb, and average posts per street and postal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avenida das AmÃ©ricas :  57\n",
      "Avenida Vieira Souto :  56\n",
      "Rua do Senado :  20\n",
      "Rua VinÃ­cius de Moraes :  19\n",
      "Avenida Embaixador Abelardo Bueno :  14\n"
     ]
    }
   ],
   "source": [
    "# Most populated streets on suburban areas \n",
    "# based on address.suburb\n",
    "pipeline = [\n",
    "    {'$match': {'address.suburb': {'$exists': 1}, 'address.street': {'$exists': 1}}},\n",
    "    {'$group': {'_id': '$address.street', 'count': {'$sum': 1}}},\n",
    "    {'$sort': {'count': -1}},\n",
    "    {'$limit': 5}\n",
    "]\n",
    "\n",
    "cursor = coll.aggregate(pipeline)\n",
    "for row in cursor:\n",
    "    print(row['_id'], ': ', row['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_posts_per_postcode :  3.20656634747\n"
     ]
    }
   ],
   "source": [
    "# Average posts per postal code\n",
    "pipeline = [\n",
    "    {'$match': {'address.postcode': {'$exists': 1}}},\n",
    "    {'$group': {'_id': '$address.postcode', 'count': {'$sum': 1}}},\n",
    "    {'$group': {'_id': 'average_posts_per_postcode', 'avg': {'$avg': '$count'}}},\n",
    "]\n",
    "\n",
    "cursor = coll.aggregate(pipeline)\n",
    "for row in cursor:\n",
    "    print(row['_id'], ': ', row['avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average_posts_per_street :  6.75888717156\n"
     ]
    }
   ],
   "source": [
    "# Average posts per street\n",
    "pipeline = [\n",
    "    {'$match': {'address.street': {'$exists': 1}}},\n",
    "    {'$group': {'_id': '$address.street', 'count': {'$sum': 1}}},\n",
    "    {'$group': {'_id': 'average_posts_per_street', 'avg': {'$avg': '$count'}}},\n",
    "]\n",
    "\n",
    "cursor = coll.aggregate(pipeline)\n",
    "for row in cursor:\n",
    "    print(row['_id'], ': ', row['avg'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion                        \n",
    "\n",
    "As stated before, the available data is centered on geocoordinates. Any other data is residual, present in only around 1% of the posts.\n",
    "\n",
    "The data was relatively clean, but thorough this project we made sure to further improve the address data by standardizing street names and postal codes.\n",
    "\n",
    "A suggestion is given of promoting a mobile app that uses cell phone resources to facilitate data entry. That would encorage new contributors, even more if the Open Street Map is perceived as relevant by its audience. A well executed marketing campaign could booster OSM's awareness amongst users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
